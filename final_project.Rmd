---
title: "STAT 253 - Final Project"
author: "Lucy Tran, Emydius Montes, Hengrui Jia"
date: "2022-12-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

## Data context {-}

“Music Dataset: 1950 to 2019,” collected in a research study in 2020 using Spotify’s Python package and other web APIs, is a dataset consisting of basic information and metadata for 28,372 lyrical tracks released from 1950 to 2019. Each row in the dataset represents a track with its name, artist, release year, genre, lyrics, main topic, and normalized metadata. Here, metadata refers to musical features of a track, such as: danceability, loudness, acousticness, instrumentalness, valence, and energy. Besides, the authors include normalized data for each of the sixteen topics a track can demonstrate, which ultimately determines its main topic. Examples of such topics are: night/time, violence, sadness, romantic, etc.

```{r}
# Library statements
library(dplyr);
library(readr);
library(broom);
library(ggplot2);
library(tidymodels);

tidymodels_prefer();

# Read in data
music <- read_csv("tcc_ceds_music.csv")

head(music)
```

## Research questions {-}

The dataset prompts three questions for further research:

1. It would be interesting to find out about general trends in music across decades. Are there any connections between a song’s release year and its musical features? 

2. The dataset contains some of the most popular music genres: blues, country, hiphop, jazz, pop, reggae, and rock. Songs of these genres can be easily identified by ear, although what makes them distinctive from each other is often unexplained. Can we identify a song’s genre based on its musical features? Which musical feature(s) contribute the most to a song’s genre? Such answers could help us understand how the music industry, or more fundamentally, our human brain, classifies songs.

3. The dataset contains a column called "topic". Although a track can have characteristics of multiple topics, this topic column tells us its main theme. In this project, we are interested in tracks whose topic is "violence". If we are to group the tracks into 7 clusters (so hopefully they will correspond to the 7 genres) based on the musical features that are most useful in identifying a song's genre (from questions #2), can these clusters tell us anything about their topics? If yes, which genre(s) does the cluster(s) whose major topic is "violence" mostly belong to? The answer to this question can suggest the genre that is the most violent in our dataset.


## Regression {-}

### Data exploration {-}

```{r}
# Clean data
music_clean_1 <- select(music, release_date, danceability, loudness, acousticness, instrumentalness, valence, energy)

head(music_clean_1)

set.seed(253)

# Creation of CV folds
music_cv <- vfold_cv(data = music_clean_1, v = 10)
```

### Methods {-}


```{r}
# Model specs
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')
  
lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
# Recipes & workflows
data_rec <- recipe(release_date ~ danceability + loudness + acousticness + instrumentalness + valence + energy, data = music_clean_1) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_corr()

ols_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec)

lasso_wf_tune <-  workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_lasso_spec_tune)
# Fit & tune models
## Fit OLS model
ols_mod <- ols_wf %>% 
  fit(data = music_clean_1)

## Fit & tune LASSO model
lasso_mod <- lasso_wf_tune %>%
  fit(data = music_clean_1)

penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = music_cv, # cv folds
  metrics = metric_set(rmse, rsq, mae),
  grid = penalty_grid # penalty grid defined above
)
```

```{r}
# GAM model spec
gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

# GAM model
gam_mod <- fit(gam_spec,
               release_date ~ s(danceability) + s(loudness) + s(acousticness) + s(instrumentalness) + s(valence) + s(energy),
               data = music_clean_1)

gam_mod %>% pluck('fit') %>% summary()

```

### Results {-}

MAE results from cross-validation of the OLS model and the best LASSO models (penalty = 0.001, and penalty = 0.1610262) are respectively 12.0784177 ± 0.04368178 years, 12.0838520 ± 0.043719815 years, and 12.11439 ± 0.04423626 years. In the best case scenario (subtracting the standard error from the mean MAE), predictions of these models would be off by 12.03473592 years, 12.040132185 years, and 12.07015374 years respectively. This means that on average, our predictions are about 12 years sooner or later than the real release year, with the OLS model having a slightly closer guess than the best LASSO models.

The residual plots against all predictors do not show any clear trend, implying that there is no bias where predictions tend to be underestimated/overestimated at some ranges of the predictors’ spectra. If we are to be strict, though, there is a slightly downward trend in the residual plot against loudness, which should prompt us to remodel this predictor using some nonlinear relationship.

Except for loudness, the edfs of all smooth terms slightly decrease. All p-values remain significantly small, and all variables continues being marked as "***" - highly significant. Our R-squared very slightly decreases (from 0.403 to 0.402), meaning the overall performance of this model (gam_mod_2) remains the same compared to gam_mod.


### Conclusions {-}

Based on our investigations so far, we decided the spline model is the best model since it has lower RMSE/MAE and higher r-squared. Our current analysis goal is to use the metrics gained from Spotify’s machine learning algorithm in order to predict the release year of a song (metrics such as danceability, loudness, etc). As a result, we are aiming more for predictive accuracy, which the spline models gives us more of. The spline model remains interpretable but its interpretability is also harder than the OLS/LASSO models because it is a lot harder to see which variables are more significant than others when all the p-values are too small for R to give us exact values.


--------------------------

## Classification {-}

### Data exploration {-}

```{r}
# Clean data
music_clean_2 <- select(music, genre, danceability, loudness, acousticness, instrumentalness, valence, energy) %>%
  mutate(genre = factor(genre))

head(music_clean_2)

set.seed(253)
```

```{r}
genre_count <- music_clean_2 %>% group_by(genre) %>% 
  summarise(n = n()) %>% mutate(freq=n/sum(n)) %>% arrange(-freq)
genre_count
```

### Methods {-}

```{r}
# Library statements
library(vip);
library(kknn)

```

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

# Recipe
data_rec <- recipe(genre ~ danceability+acousticness+loudness+instrumentalness+valence+energy, data = music_clean)

# Workflows
## Create workflows for mtry = 2, 3, 4, 5, and 6
data_wf_mtry1 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 1)) %>%
  add_recipe(data_rec)

data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(data_rec)

data_wf_mtry3 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 3)) %>%
  add_recipe(data_rec)

data_wf_mtry4 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 4)) %>% 
  add_recipe(data_rec)

data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5)) %>% 
  add_recipe(data_rec)

data_wf_mtry6 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 6)) %>% 
  add_recipe(data_rec)

```

```{r}
# Fit Models
# Fit models for mtry = 2, 3, 4, 5, and 6
set.seed(253) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry1 <- fit(data_wf_mtry1, data = music_clean)

set.seed(253)
data_fit_mtry2 <- fit(data_wf_mtry2, data = music_clean)

set.seed(253)
data_fit_mtry3 <- fit(data_wf_mtry3, data = music_clean)

set.seed(253) 
data_fit_mtry4 <- fit(data_wf_mtry4, data = music_clean)

set.seed(253)
data_fit_mtry5 <- fit(data_wf_mtry5, data = music_clean)

set.seed(253) 
data_fit_mtry6 <- fit(data_wf_mtry6, data = music_clean)

```
```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          class = truth,
          label = model_label
      )
}

#check out the function output
rf_OOB_output(data_fit_mtry1,1, music_clean %>% pull(genre))
```



```{r}
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry1,1, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry2,2, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry3,3, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry4,4, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry5,5, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry6,6, music_clean %>% pull(genre))
)

data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = class, estimate = .pred_class)
```


```{r}
labels <- data_rf_OOB_output %>% 
    select(label)
accuracys <- data_rf_OOB_output %>% 
    group_by(label) %>% 
    accuracy(truth = class,estimate= .pred_class) 

ggplot(accuracys,aes(x= label,y=.estimate)) +
  geom_point() +
  geom_line() +
  theme_classic()
```

```{r}
# Model Specification
mtry1_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 1, # size of random subset of variables
           trees = 1000, # Number of trees
           min_n = NULL, # tuning parameter is min_n; tuning spec
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

mtry2_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 2, # size of random subset of variables
           trees = 1000, # Number of trees
           min_n = NULL, # tuning parameter is min_n; tuning spec
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

# min_n values
min_ns <- seq(5, 100, 5) # 20 values of min_n, from 5 to 100 with step of 5

# Custom function to get OOB result of a random forest model with custom tuning min_n
OOB_output <- function(model_spec, min_ns) {
  mtry_OOB_output <- bind_rows()
  for (min_n in min_ns) {
    # Workflow (Recipe + Model)
    main_wf <- workflow() %>% 
      add_model(model_spec %>% set_args(min_n = min_n)) %>%
      add_recipe(data_rec)
    
    set.seed(253)
    fit_model <- fit(main_wf, data = music_clean)
    mtry_OOB_output <- bind_rows(mtry_OOB_output, 
                                 rf_OOB_output(fit_model, min_n, music_clean %>% pull(genre)))
  }
  return (mtry_OOB_output %>% 
            group_by(label) %>% 
            accuracy(truth = class, estimate = .pred_class))
}

# Result
mtry1_result <- OOB_output(mtry1_spec, min_ns)
mtry2_result <- OOB_output(mtry2_spec, min_ns)
```

```{r}
# Finalize the best random forest model
best_rf_wf <- workflow() %>%
  add_model(mtry2_spec %>% set_args(min_n = 70)) %>% # choose the best mtry and the best min_n
  add_recipe(data_rec)

best_rf_fit <- fit(best_rf_wf, data = music_clean)
```

```{r}
# Impurity approach
impurity_output <- best_rf_fit %>% 
    extract_fit_engine() 

impurity_output %>% 
    vip(num_features = 30) + theme_classic() #based on impurity

impurity_output %>% vip::vi() %>% head()
```

```{r}
# Permutation approach
permutation_output <- best_rf_fit %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = music_clean) %>% 
    extract_fit_engine() 

permutation_output %>% 
    vip(num_features = 30) + theme_classic()


permutation_output %>% vip::vi() %>% head()
```

```{r}
# CV Folds
data_cv10 <- vfold_cv(music_clean, v = 10)

# Model Specification
knn_spec <- 
  nearest_neighbor() %>% # new type of model!
  set_args(neighbors = tune()) %>% # tuning parameter is neighbor; tuning spec
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('classification') 

# Recipe with standardization (!)
data_rec <- recipe( genre ~ danceability+loudness+acousticness+instrumentalness+valence+energy , data = music_clean) %>%
    step_nzv(all_predictors())# removes variables with the same value

# Workflow (Recipe + Model)
knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(data_rec)

# Tune model trying a variety of values for neighbors (using 10-fold CV)
penalty_grid <- grid_regular(
  neighbors(range = c(1, 500)), #  min and max of values for neighbors
  levels = 50) # number of neighbors values

knn_fit_cv <- tune_grid(knn_wf, # workflow
              resamples = data_cv10, #CV folds
              grid = penalty_grid, # grid specified above
              metrics = metric_set(accuracy))
```

```{r}
# Variable importance
orig_accuracy <- 0.3743129

spec_wo_var <- nearest_neighbor(neighbors=153) %>% # new type of model!
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('classification') 

calculate_var_importance <- function(var) {
  new_data <- music_clean %>% select(-var)                                                          
  rec_wo_var <- recipe(genre ~ ., data = new_data)
  
  # Workflow (Recipe + Model)
  wf_wo_var <- workflow() %>%
    add_model(spec_wo_var) %>% 
    add_recipe(rec_wo_var)
  
  knn_mod_wo_var <- fit_resamples(wf_wo_var,
    resamples = data_cv10, 
    metrics = metric_set(yardstick::accuracy))

  accuracy_wo_var <- knn_mod_wo_var %>% collect_metrics()
  return (c(var, orig_accuracy - accuracy_wo_var %>% pull(mean)))
}

result <- data.frame(matrix(ncol=2,nrow=6, dimnames=list(NULL, c("variable", "difference"))))

result[1,] <- calculate_var_importance('danceability')
result[2,] <- calculate_var_importance('energy')
result[3,] <- calculate_var_importance('instrumentalness')
result[4,] <- calculate_var_importance('acousticness')
result[5,] <- calculate_var_importance('valence')
result[6,] <- calculate_var_importance('loudness') 

result %>% arrange(difference)
```

### Results {-}

The best overall model with the highest accuracy is the random forest model with `mtry` = 2 and `min_n` = 70. Although it is our best model, its OOB accuracy estimation is only 39.55%, which is quite low for a prediction model. However, given that we have up to 7 genres to classify, and given how all our predictors - `danceability`, `energy`, `acousticness`, `instrumentalness`, `valance`, `loudness` are not strictly sonic-based but calculated based on an undisclosed formula/algorithm of Spotify, this level of accuracy is quite good. 



### Conclusions {-}

The evaluation metric we got for the final model was 39.55% accuracy with a random forest of `mtry` = 2 and `min_n` = 70. This means that using our model, we were able to predict the correct genre given our metrics about 39.55% of the time. This isn't a great amount of accuracy, since it means 60% of the time we'll get something that is not correct. However, this model is accurate enough to be significant in predicting genre. The NIR for our data was 24.82%, which we got from the largest category in our data set, being the pop genre. If our model had guessed every single song was pop, the accuracy would have been 24.82%, so the model has to exceed this in order for it to have any significance in classifying genre. Our accuracy, 39.55% is 1.59 times more accurate than the NIR meaning it's at least significantly more accurate in predicting genre than straight up guessing would be. Still, it doesn't exceed 50%, meaning the model would be wrong more times than it is correct. 


According to our random forest model's permutation measurement, the ranking of variable importance is as follows: danceability, energy, instrumentalness, acousticness, valence, and loudness. However, both the permutation and impurity measurements agree that the most important variable is `danceability`, and the two least important variable is `valance` and `loudness`. This means that there is little to no difference in loudness and valence (how positive or negative a track sounds) among the seven genres (blues, country, hip hop, jazz, pop, reggae, rock). To see how each of the features singles out different genres, we can look at the average values of the features that each genre demonstrates, as well as the violin plots:


--------------------------

## Unsupervised Learning {-}

### Data exploration {-}

First, let's explore the distribution of topics in our dataset:

```{r}
topic_count <- music %>% group_by(topic) %>% 
  summarise(n = n()) %>% mutate(freq=n/sum(n)) %>% arrange(-freq)
topic_count
```

There are 8 topics: sadness, violence, world/life, obscene, music, night/time, romantic, and feelings. The former 4 topics in the same order are the more prevalent ones. Since we want to relate these topics with the 7 genres, we will use only the musical features most important in classifying genres to do clustering, which, according to the result of research question #2, are: danceability, energy, instrumentalness, and acousticness. Reducing these number of variables will help avoid overfitting, and reduce computational resource while maintaining the predictive power. To perform clustering, we choose K-means over hierarchical clustering, because we want there to be 7 clusters corresponding to the 7 genres. Another reason is that we do not need to interpret the distances between tracks, because there are too many of them, and because eventually, we are not interested in how similar the tracks are in a cluster (except when tuning for the best model).


### Method {-}

```{r}
music_clean_3 <- music

# Select only the most important musical features in identifying tracks' genres
# Musical feature variables in the original dataset are already standardized, so we do not need to do that here. 
music_clean_3_sub <- music_clean_3 %>% select(danceability, acousticness, instrumentalness, energy)
```

```{r}
# Set the seed before performing clustering to get repeatable results
set.seed(253)

# Group the tracks into 7 clusters (based on cluster centroids)
kclust_k7 <- kmeans(music_clean_3_sub, centers = 7)

# Assign clusters to tracks
music_clean_3 <- music_clean_3 %>%
    mutate(kclust_7 = factor(kclust_k7$cluster))
```

```{r}
# Visualize the cluster assignments, labeled by topic
ggplot(music_clean_3, aes(x = kclust_7, fill = topic)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
```

As seen in the 1st graph, in cluster 4 and cluster 6, the most prevalent topic is "violence". We will check to see if these 2 clusters account for most of the tracks whose observed topic is "violence".

```{r}
# Visualize the 8 topics, labeled by cluster assignment
ggplot(music_clean_3, aes(x = topic, fill = kclust_7)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
```

Interestingly, only cluster 4 makes up more than 25% of "violence" tracks. The second most common cluster is cluster 1, but compared to other topics, "violence" is not the most, even the second most, prevalent topic for cluster 1. While cluster 6 makes up only a small part of every topic, its most common topic is clearly "violence".

Next, we look at the genres of these clusters, with more attention to cluster 4 and 6.

```{r}
# Visualize cluster assignments, labeled by genre
ggplot(music_clean_3, aes(x = kclust_7, fill = genre)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
```

In cluster 4, we see 3 genres make up the most of it: rock, pop, and blues in the order of their proportion. In cluster 6, we also see 3 most common genres: jazz, blues, and rock. So, rock and blues are the 2 overlapping common genre of these two clusters. 

Let's also look at the a visualization of cluster composition of the 7 genres to gain more insights:

```{r}
ggplot(music_clean_3, aes(x = genre, fill = kclust_7)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
```

Interestingly, rock has up to about 50% of its tracks belonging to cluster 4 - the "most violent cluster". If we combine cluster 4 and 6 in each genre, we can see that blues, pop, and jazz in order also has a large number of tracks from cluster 4 and 6 (compared to country, hip hop, and reggae), but not as many as rock. At this point, we can tentatively conclude that rock, blues, pop, and jazz in order are the genres that tend to be violent the most.

However, this result may happen by chance, because in K-means, the algorithms randomly assign 7 clusters at the beginning, which means the final cluster assignments may have been different if we use a different seed. Therefore, before we make the final conclusion, we will run the algorithm with different seeds to have different random initializations. The best initialization will have the smallest within-cluster sum of squares.


### Choosing the best random initialization {-}

Because we have a very large dataset, we will test with 100 different seeds to make sure we will get the best seeds within the test range.

```{r}
# Data-specific function to cluster and calculate total within-cluster sum of squares
music_cluster_ss <- function(seed){
  set.seed(seed)
  
  # Perform clustering
  kclust <- kmeans(music_clean_3_sub, centers = 7)

  # Return the total within-cluster sum of squares
  return(kclust$tot.withinss)
}

results <- tibble(
    k = 1:100,
    total_wc_ss = purrr::map_dbl(1:100, music_cluster_ss)
)

results %>% 
    ggplot(aes(x = k, y = total_wc_ss)) +
    geom_point() + 
    labs(x = "Seed",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

```{r}
results %>% slice_min(total_wc_ss)
```

As it turns out, initializations with seed = 39, 43, 55, 67, 72, 86, 93 are the best. Their total within-cluster sum of squares are the same, at 1500.301.


### Conclusion with the best initializations

We will now test our tentative conclusions above (with seed = 253) with 3 of the best seeds. Ideally, whatever the seed is, we want to have 2 clusters whose most common topic is violence, and within these 2 clusters, the most prevalent genres are rock, blues, pop, and jazz.

```{r}
k_means <- function() {
  kclust_k7 <- kmeans(music_clean_3_sub, centers = 7)

  music_clean_3 <- music_clean_3 %>%
    mutate(kclust_7 = factor(kclust_k7$cluster))
  
  topic <- ggplot(music_clean_3, aes(x = kclust_7, fill = topic)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
  
  show(topic)
  
  genre <- ggplot(music_clean_3, aes(x = kclust_7, fill = genre)) +
    geom_bar(position = "fill") +
    labs(x = "Cluster") + 
    theme_classic()
  
  show(genre)
}
```


```{r}
set.seed(39)
k_means()
```

With seed = 39, K-means gives us all the same conclusions as with seed = 253, although the exact numbers may be slightly different. Cluster 1 and 4 are the "most violent clusters". In cluster 1, jazz, blues, and rock are the most common genres (same as cluster 6 when seed = 253). In cluster 4, rock, pop, and blues are the most common genres (same as cluster 4 when seed = 253). The order of prevalence of these genres within each of these two cluster remains the same.

```{r}
set.seed(43)
k_means()
```

With seed = 43, the conclusions remain the same. Cluster 2 here corresponds to cluster 4 when seed = 253. Cluster 3 here corresponds to cluster 6 when seed = 253.

```{r}
set.seed(67)
k_means()
```

Again, the conclusions remain the same with seed = 67. Cluster 1 here corresponds to cluster 4 when seed = 253. Cluster 3 here corresponds to cluster 6 when seed = 253.


### Conclusions {-}


