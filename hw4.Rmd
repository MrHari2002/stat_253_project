---
title: "STAT 253 HW4"
author: "Emydius, Hari, Lucy"
date: "2022-11-1"
output: html_document
---

```{r hw4_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

# Homework 4 {-} 

## Research question {-} 

**Data context:**

“Music Dataset: 1950 to 2019,” collected in a research study in 2020 using Spotify’s Python package and other web APIs, is a dataset consisting of basic information and metadata for 28,372 lyrical tracks released from 1950 to 2019. Each row in the dataset represents a track with its name, artist, release year, genre, lyrics, main topic, and normalized metadata. Here, metadata refers to musical features of a track, such as: danceability, loudness, acousticness, instrumentalness, valence, and energy. Besides, the authors also include normalized data for each of the sixteen topics a track can demonstrate, which ultimately determines its main topic. Examples of such topics are: dating, violence, sadness, romantic, etc.

> The dataset contains some of the most popular music genres: blues, country, hiphop, jazz, pop, reggae, and rock. Songs of these genres can be easily identified by ear, although what makes them distinctive from each other is often unexplained. **Can we identify a song’s genre based on its musical features? Which musical feature(s) contribute the most to a song’s genre?** Such answers could help us understand how the music industry, or more fundamentally, our human brain, classifies songs.


## Data Exploration {-}

First, let's explore our data in order to shape our model choice.

```{r}
# Library statements
library(dplyr);
library(readr);
library(broom);
library(ggplot2);
library(tidymodels);
library(vip);
library(kknn)
tidymodels_prefer();

# Read in data
music <- read_csv("tcc_ceds_music.csv")
# Clean data
music_clean <- select(music, genre, danceability, loudness, acousticness, instrumentalness, valence, energy) %>%
  mutate(genre = factor(genre))

head(music_clean)

set.seed(253)
```

```{r}
genre_count <- music_clean %>% group_by(genre) %>% 
  summarise(n = n()) %>% mutate(freq=n/sum(n)) %>% arrange(-freq)
genre_count
```

There are 7 genres in our dataset: blues, country, hip hop, jazz, pop, reggae, and rock. Pop has the largest frequency among all 7 genres at 24.8%; hip hop has the lowest frequency at 3.1%, and reggae has the second lowest frequency at 8.8%.

Because our outcome variable has 7 possible values, we cannot use logistic regression, which is used for binary classification. Hence, we will be answering our research question using random forest and K-nearest neighbors. 

However, we note that since all our predictors - danceability, loudness, acousticness, instrumentalness, valence, and energy, are all numerical, it will be highly computationally intensive to perform random forest because there are so many possible splits to consider. And although KNN generally has low computational time because of its lazy learning approach, when paired with cross-validation and tune for the best K, it can be computationally heavy as well. 


## Methods {-}

### Random Forest {-}

#### Fitting Models {-}

For our models specification, we'll start with the random forests of 1000 trees, and our trees will be complex with `min_n` of 2 (so all the leaf nodes will have at most 10 cases). To make our trees diverse and avoid overfitting (when all trees are too similar), each tree will have different predictors to consider at each split. This number of predictors also identifies the different forests we will test out to determine which `mtry` is best for our overall accuracy. Since we only have 6 predictors, we will test out every possible `mtry` and see which one has the best overall accuracy.

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

# Recipe
data_rec <- recipe(genre ~ danceability+acousticness+loudness+instrumentalness+valence+energy, data = music_clean)

# Workflows
## Create workflows for mtry = 2, 3, 4, 5, and 6
data_wf_mtry1 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 1)) %>%
  add_recipe(data_rec)

data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(data_rec)

data_wf_mtry3 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 3)) %>%
  add_recipe(data_rec)

data_wf_mtry4 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 4)) %>% 
  add_recipe(data_rec)

data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5)) %>% 
  add_recipe(data_rec)

data_wf_mtry6 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 6)) %>% 
  add_recipe(data_rec)
```

```{r}
# Fit Models
# Fit models for mtry = 2, 3, 4, 5, and 6
set.seed(253) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry1 <- fit(data_wf_mtry1, data = music_clean)

set.seed(253)
data_fit_mtry2 <- fit(data_wf_mtry2, data = music_clean)

set.seed(253)
data_fit_mtry3 <- fit(data_wf_mtry3, data = music_clean)

set.seed(253) 
data_fit_mtry4 <- fit(data_wf_mtry4, data = music_clean)

set.seed(253)
data_fit_mtry5 <- fit(data_wf_mtry5, data = music_clean)

set.seed(253) 
data_fit_mtry6 <- fit(data_wf_mtry6, data = music_clean)
```

#### Evaluation {-}

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          class = truth,
          label = model_label
      )
}

#check out the function output
rf_OOB_output(data_fit_mtry1,1, music_clean %>% pull(genre))
```



```{r}
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry1,1, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry2,2, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry3,3, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry4,4, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry5,5, music_clean %>% pull(genre)),
    rf_OOB_output(data_fit_mtry6,6, music_clean %>% pull(genre))
)

data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = class, estimate = .pred_class)
```


```{r}
labels <- data_rf_OOB_output %>% 
    select(label)
accuracys <- data_rf_OOB_output %>% 
    group_by(label) %>% 
    accuracy(truth = class,estimate= .pred_class) 

ggplot(accuracys,aes(x= label,y=.estimate)) +
  geom_point() +
  geom_line() +
  theme_classic()
```

As it turns out, `data_fit_mtry1` gives us the best overall accuracy estimate, that is, 38.6%. Accuracy drops with a decreasing speed as we increase `mtry` to 2, 3, 4, and 5. Then, when `mtry` = 6, we have a slightly better accuracy compared to `data_fit_mtry5`. 

Considering the bias-variance tradeoff, although `mtry` = 1 leads to very high variance in the decisions among the trees, we know that in the end, when counting the majority votes from all trees in the forest, this model still wins as the one whose majority votes are closest to the truth. This may be due to the fact that `data_fit_mtry1` was able to include splits on "weaker" variables that normally would not "win" in any splits. Thus, `data_fit_mtry1` has more chance to explore the globally best combination of splits, rather than locally best ones that prioritize strong variables or variables that are more correlated with the previously chosen one. 

However, there is another tuning parameter we should consider - `min_n`, the minimum number of cases in a node to consider splitting. Since `data_fit_mtry2`'s accuracy is very close to that of `data_fit_mtry1`, we will tune for the best `min_n` on both of these values of `mtry`. It is possible that with a different value of `min_n`, `mtry` = 2 is actually the best one.

#### Tuning for the best min_n {-}

```{r}
# Model Specification
mtry1_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 1, # size of random subset of variables
           trees = 1000, # Number of trees
           min_n = NULL, # tuning parameter is min_n; tuning spec
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

mtry2_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 2, # size of random subset of variables
           trees = 1000, # Number of trees
           min_n = NULL, # tuning parameter is min_n; tuning spec
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') 

# min_n values
min_ns <- seq(5, 100, 5) # 20 values of min_n, from 5 to 100 with step of 5

# Custom function to get OOB result of a random forest model with custom tuning min_n
OOB_output <- function(model_spec, min_ns) {
  mtry_OOB_output <- bind_rows()
  for (min_n in min_ns) {
    # Workflow (Recipe + Model)
    main_wf <- workflow() %>% 
      add_model(model_spec %>% set_args(min_n = min_n)) %>%
      add_recipe(data_rec)
    
    set.seed(253)
    fit_model <- fit(main_wf, data = music_clean)
    mtry_OOB_output <- bind_rows(mtry_OOB_output, 
                                 rf_OOB_output(fit_model, min_n, music_clean %>% pull(genre)))
  }
  return (mtry_OOB_output %>% 
            group_by(label) %>% 
            accuracy(truth = class, estimate = .pred_class))
}

# Result
mtry1_result <- OOB_output(mtry1_spec, min_ns)
mtry2_result <- OOB_output(mtry2_spec, min_ns)
```


```{r}
# Bind mtry column
mtry1_result$mtry <- rep_len(1, length(min_ns))
mtry2_result$mtry <- rep_len(2, length(min_ns))

accuracys <- bind_rows(mtry1_result, mtry2_result)

# Visualize how the accuracies of mtry1 and mtry2 change across different min_n's 
ggplot(accuracys,aes(x=label, y=.estimate, group=mtry, color=factor(mtry))) +
  geom_point() +
  geom_line() +
  xlab("min_n") + ylab("Accuracy estimate") +
  theme_classic()

# Show the random forest model with the best overall accuracy
accuracys %>% slice_max(.estimate)
```

As seen from the graph, at about `min_n`=30, the OOB accuracy of the `mtry2` model starts to get higher than that of the `mtry1` model. The highest accuracy we can get is 39.55%, which is from a random forest with `mtry` = 2 and `min_n` = 70. We can also try highest values of `mtry`, but based on the little difference in accuracy among different `mtry`'s at the same `min_n` as we've seen above, we do not expect a drastic increase in accuracy even if there is. Plus, as discussed above, picking higher `mtry` (3, 4, 5 and 6) may actually hurt our chance of building trees that include the globally best combinations of splits, given how few predictors we have. 

Therefore, we will move forward to examining variable importance with the best random forest model we have concluded.

#### Variable Importance {-}

```{r}
# Finalize the best random forest model
best_rf_wf <- workflow() %>%
  add_model(mtry2_spec %>% set_args(min_n = 70)) %>% # choose the best mtry and the best min_n
  add_recipe(data_rec)

best_rf_fit <- fit(best_rf_wf, data = music_clean)
```

```{r}
# Impurity approach
impurity_output <- best_rf_fit %>% 
    extract_fit_engine() 

impurity_output %>% 
    vip(num_features = 30) + theme_classic() #based on impurity

impurity_output %>% vip::vi() %>% head()
```

```{r}
# Permutation approach
permutation_output <- best_rf_fit %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = music_clean) %>% 
    extract_fit_engine() 

permutation_output %>% 
    vip(num_features = 30) + theme_classic()


permutation_output %>% vip::vi() %>% head()
```

Both measurements agree on the the rank of `danceability` (most important), `valence` (5th rank), and `loudness` (6th, last rank) but disagree on the rank of `energy`, `instrumentalness` and `acousticness`. Impurity ranks `energy` lower than `instrumentalness` and acousticness`, while permutation ranks `energy` higher than both.

However, we know that the impurity measure tends to favor predictors with a lot of unique values, because with many unique values, there are many more possible splits, thus making it easier to find an optimal split that makes a big improvement in purity. Therefore, we believe that the result from the permutation approach is more reliable. 

To find out whether this is actually the case, we will take a look at the number of unique values of these predictors: 

```{r}
# Count the number of unique values of each predictor
music_clean %>% select(instrumentalness, acousticness, energy) %>%
  sapply(function (x) n_distinct(x))
```

As it turns out, impurity may have actually favored predictors with more unique values - `instrumentalness` has 3.66 times more unique values than `energy`, while `acousticness` has 2.81 more unique values than `energy`. Therefore, the variable importance rankings of permutation is more likely to be accurate.

Next, we will use an alternative model type to answer our research questions.


### K-nearest Neighbors {-}

#### Fitting Models {-}

To find the best K - the number of neighbors, we will start with a tuning grid of 50 values of K from 1 to 500. We will also use cross validation to avoid overfitting and get the true test error.

```{r}
# CV Folds
data_cv10 <- vfold_cv(music_clean, v = 10)

# Model Specification
knn_spec <- 
  nearest_neighbor() %>% # new type of model!
  set_args(neighbors = tune()) %>% # tuning parameter is neighbor; tuning spec
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('classification') 

# Recipe with standardization (!)
data_rec <- recipe( genre ~ danceability+loudness+acousticness+instrumentalness+valence+energy , data = music_clean) %>%
    step_nzv(all_predictors())# removes variables with the same value

# Workflow (Recipe + Model)
knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(data_rec)

# Tune model trying a variety of values for neighbors (using 10-fold CV)
penalty_grid <- grid_regular(
  neighbors(range = c(1, 500)), #  min and max of values for neighbors
  levels = 50) # number of neighbors values

knn_fit_cv <- tune_grid(knn_wf, # workflow
              resamples = data_cv10, #CV folds
              grid = penalty_grid, # grid specified above
              metrics = metric_set(accuracy))
```

#### Evaluation {-}

```{r}
knn_fit_cv %>% autoplot()
```

```{r}
knn_fit_cv %>% show_best(metric = 'accuracy') # Show evaluation metrics for different values of neighbors, ordered

knn_fit_cv %>% 
  select_by_one_std_err(metric = 'accuracy', desc(neighbors))  # Choose neighbors value that leads to the highest neighbors within 1 se of the lowest CV MAE
```

The best KNN model includes 174 neighbors, and has an overall test accuracy of 37.53% with a standard error of 0.4%. If we want to decrease variance, then the model with the highest number of neighbors within 1 standard error of the lowest cross-validated MAE is the one with 500 neighbors, which gives an overall test accuracy of 37.25% with a standard error of 0.42%. As seen from the graph, accuracy will tend to go down, or not pass the threshold of 38% if we keep increasing the number of neighbors. Therefore, we believe the best KNN model (out of all possible KNN models) has less accuracy than the best random forest model (39.55%).


#### Variable Importance {-}

```{r}
# Variable importance
orig_accuracy <- 0.3743129

spec_wo_var <- nearest_neighbor(neighbors=153) %>% # new type of model!
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('classification') 

calculate_var_importance <- function(var) {
  new_data <- music_clean %>% select(-var)                                                          
  rec_wo_var <- recipe(genre ~ ., data = new_data)
  
  # Workflow (Recipe + Model)
  wf_wo_var <- workflow() %>%
    add_model(spec_wo_var) %>% 
    add_recipe(rec_wo_var)
  
  knn_mod_wo_var <- fit_resamples(wf_wo_var,
    resamples = data_cv10, 
    metrics = metric_set(yardstick::accuracy))

  accuracy_wo_var <- knn_mod_wo_var %>% collect_metrics()
  return (c(var, orig_accuracy - accuracy_wo_var %>% pull(mean)))
}

result <- data.frame(matrix(ncol=2,nrow=6, dimnames=list(NULL, c("variable", "difference"))))

result[1,] <- calculate_var_importance('danceability')
result[2,] <- calculate_var_importance('energy')
result[3,] <- calculate_var_importance('instrumentalness')
result[4,] <- calculate_var_importance('acousticness')
result[5,] <- calculate_var_importance('valence')
result[6,] <- calculate_var_importance('loudness') 

result %>% arrange(difference)
```

While random forest's rankings KNN's rankings disagree on the ranks of most variables, they both agree that danceability is the most important predictor, and loudness is the least important predictor. KNN ranks the predictors significance as follows, from most important to least important:  


## Results

The best overall model with the highest accuracy is the random forest model with `mtry` = 2 and `min_n` = 70. Although it is our best model, its OOB accuracy estimation is only 39.55%, which is quite low for a prediction model. However, given that we have up to 7 genres to classify, and given how all our predictors - `danceability`, `energy`, `acousticness`, `instrumentalness`, `valance`, `loudness` are not strictly sonic-based but calculated based on an undisclosed formula/algorithm of Spotify, this level of accuracy is quite good. 

```{r}
accuracys %>% slice_max(.estimate)
```

Since this is the model with the highest accuracy, we also take its variable importance ranking to be the most reliable. Of the two measurements for variable importance in random forest models, we believe the ranking by the permutation approach is the more accurate, because impurity tends to favor variables with more unique values - which seems to be what actually happened in the variables whose rankings the two measurements disagree on. Therefore, the orders of importance among the variables is as follows: danceability, energy, instrumentalness, acousticness, valence, and loudness.

```{r}
permutation_output %>% vip::vi() %>% head()
```


## Conclusions {-}

#### Comparing With the No-Information Rate (NIR) {-}

The evaluation metric we got for the final model was 39.55% accuracy with a random forest of `mtry` = 2 and `min_n` = 70. This means that using our model, we were able to predict the correct genre given our metrics about 39.55% of the time. This isn't a great amount of accuracy, since it means 60% of the time we'll get something that is not correct. However, this model is accurate enough to be significant in predicting genre. The NIR for our data was 24.82%, which we got from the largest category in our data set, being the pop genre. If our model had guessed every single song was pop, the accuracy would have been 24.82%, so the model has to exceed this in order for it to have any significance in classifying genre. Our accuracy, 39.55% is 1.59 times more accurate than the NIR meaning it's at least significantly more accurate in predicting genre than straight up guessing would be. Still, it doesn't exceed 50%, meaning the model would be wrong more times than it is correct. 

#### Variable Importance Interpretation {-}

According to our random forest model's permutation measurement, the ranking of variable importance is as follows: danceability, energy, instrumentalness, acousticness, valence, and loudness. However, both the permutation and impurity measurements agree that the most important variable is `danceability`, and the two least important variable is `valance` and `loudness`. This means that there is little to no difference in loudness and valence (how positive or negative a track sounds) among the seven genres (blues, country, hip hop, jazz, pop, reggae, rock). To see how each of the features singles out different genres, we can look at the average values of the features that each genre demonstrates, as well as the violin plots:

```{r}
# Average values of features
music_clean %>%
  group_by(genre) %>% 
  summarize(avg_danceability=mean(danceability), avg_energy=mean(energy), avg_instrumentalness=mean(instrumentalness),avg_acousticness=mean(acousticness), avg_valence=mean(valence), avg_loudness=mean(loudness))

# Violin plots of the predictors, in the order of permutation measurement
ggplot(music_clean, aes(x = genre, y = danceability)) + 
    geom_violin() + theme_classic()
ggplot(music_clean, aes(x = genre, y = energy)) + 
    geom_violin() + theme_classic()
ggplot(music_clean, aes(x = genre, y = instrumentalness)) + 
    geom_violin() + theme_classic()

ggplot(music_clean, aes(x = genre, y = acousticness)) + 
    geom_violin() + theme_classic()
ggplot(music_clean, aes(x = genre, y = valence)) + 
    geom_violin() + theme_classic()
ggplot(music_clean, aes(x = genre, y = loudness)) + 
    geom_violin() + theme_classic()
```

From the average scores above, there are several observations:
  - Hip hop and reggae are much more `danceable` than other genres. This makes sense, because these 2 genres are commonly known as "dance music".
  - Hip hop and rock score quite noticeably higher on `energy`. than other genres.
  - Jazz has the highest `instrumentalness` (~less vocal), but the difference to the second most instrumental genre is only 0.23357732	- 0.09453841 = 0.13903891. After jazz, blues and rock are also twice or more as instrumental as the remaining genres.
  - Hip hop, reggae, and rock are generally much less `acoustic`.
  - Again, hip hop and reggae score the highest on `valence`, although not as evident as with danceability.
  - `Loudness` does not differ much among the genres.
  
Both the average feature values and the violin plots are generally in agreement with our final ranking of variable importance. 

#### Confusion Matrix {-}

```{r}
# Confusion Matrix
rf_OOB_output(best_rf_fit, "mtry = 2, min_n = 70", music_clean %>% pull(genre)) %>%
  conf_mat(truth = class, estimate = .pred_class)
```

Looking at the confusion matrix, we can see that except for blues, hip hop and rock, our best model predicted the correct genre more times than each of the incorrect genres. Looking at actual numbers, interpretations are difficult to make given that the proportions for each genre were not equal and varied wildly. However, here, we will list out the genres that our model misidentified the most for each of the true observed genres:

  - blues: commonly misidentified as **pop** and country.
  - country: commonly misidentified as pop
  - hip hop: commonly misidentified as **pop**, **reggae**
  - jazz: commonly misidentified as country, pop
  - pop: commonly misidentified as country
  - reggae: commonly misidentified as pop, country
  - rock: commonly misidentified as **pop**
  
Note that the bolded genres are the ones that get predicted even more times than the true genre.

One interesting observation we can make is that for when the model predicted reggae, it turned out to be hip hop more times than jazz and blues, despite hip hop having the lowest proportion of songs (904 songs vs jazz's 3845 songs and blues' 4604 songs) meaning it overpredicted reggae for hip hop in comparison to other false genres. This makes sense given how hip hop and reggae both has generally high danceability, high valance, and low acousticness. 




